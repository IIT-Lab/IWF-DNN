{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/user/Desktop/IWF-DNN/data2DComplex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda2/envs/haha/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/user/Desktop/IWF-DNN/data2DComplex\n",
    "%pwd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import print_function\n",
    "import scipy.io as sio\n",
    "import time\n",
    "from sklearn.cross_validation import train_test_split\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DateLocation='MIMO100000_3.mat'\n",
    "Data = sio.loadmat(DateLocation)#load data\n",
    "Xdata=Data['X']\n",
    "Ydata=Data['Y']\n",
    "Xdata = np.transpose(Xdata)\n",
    "Ydata = np.transpose(Ydata)\n",
    "X = np.column_stack((np.real(Xdata), np.imag(Xdata)))\n",
    "Y = np.column_stack((np.real(Ydata), np.imag(Ydata)))\n",
    "x_train, x_val ,y_train, y_val= train_test_split(X, Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_whole(dict_data):\n",
    "    fig_1 = plt.figure(figsize=(16,5))\n",
    "    ax_1 = fig_1.add_subplot(1,2,1)\n",
    "    ax_2 = fig_1.add_subplot(1,2,2)\n",
    "    k =int(0)\n",
    "    for data in dict_data.values():\n",
    "        ax_1.plot(np.arange(0,len(data['loss'])),data['loss'], label=list(dict_data.keys())[k])\n",
    "        ax_2.plot(np.arange(0,len(data['val_loss'])),data['val_loss'], label=list(dict_data.keys())[k])\n",
    "        k += 1\n",
    "    ax_1.legend(loc=0)\n",
    "    \n",
    "    ax_2.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "    ax_1.set_ylabel('error(train)')\n",
    "    ax_2.set_xlabel('Epoch number')\n",
    "    ax_2.set_ylabel('error(valid)')\n",
    "    fig_1.savefig('err.pdf')\n",
    "    return fig_1, ax_1, ax_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 600)               15000     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 480)               288480    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 240)               115440    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 120)               28920     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 60)                7260      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 24)                1464      \n",
      "=================================================================\n",
      "Total params: 456,564\n",
      "Trainable params: 456,564\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "90000/90000 [==============================] - 30s 328us/step - loss: 0.6080 - val_loss: 0.3977\n",
      "Epoch 2/200\n",
      "90000/90000 [==============================] - 29s 327us/step - loss: 0.3446 - val_loss: 0.3183\n",
      "Epoch 3/200\n",
      "90000/90000 [==============================] - 26s 294us/step - loss: 0.2697 - val_loss: 0.2787\n",
      "Epoch 4/200\n",
      "90000/90000 [==============================] - 26s 287us/step - loss: 0.2252 - val_loss: 0.2443\n",
      "Epoch 5/200\n",
      "90000/90000 [==============================] - 30s 331us/step - loss: 0.1969 - val_loss: 0.2375\n",
      "Epoch 6/200\n",
      "90000/90000 [==============================] - 26s 291us/step - loss: 0.1769 - val_loss: 0.2266\n",
      "Epoch 7/200\n",
      "90000/90000 [==============================] - 22s 250us/step - loss: 0.1609 - val_loss: 0.2223\n",
      "Epoch 8/200\n",
      "90000/90000 [==============================] - 23s 251us/step - loss: 0.1474 - val_loss: 0.2189\n",
      "Epoch 9/200\n",
      "90000/90000 [==============================] - 22s 243us/step - loss: 0.1373 - val_loss: 0.2112\n",
      "Epoch 10/200\n",
      "90000/90000 [==============================] - 23s 251us/step - loss: 0.1277 - val_loss: 0.2013\n",
      "Epoch 11/200\n",
      "90000/90000 [==============================] - 21s 239us/step - loss: 0.1184 - val_loss: 0.1977\n",
      "Epoch 12/200\n",
      "90000/90000 [==============================] - 24s 271us/step - loss: 0.1115 - val_loss: 0.1861\n",
      "Epoch 13/200\n",
      "90000/90000 [==============================] - 29s 324us/step - loss: 0.1049 - val_loss: 0.1903\n",
      "Epoch 14/200\n",
      "90000/90000 [==============================] - 27s 297us/step - loss: 0.0990 - val_loss: 0.1856\n",
      "Epoch 15/200\n",
      "90000/90000 [==============================] - 28s 310us/step - loss: 0.0944 - val_loss: 0.1844\n",
      "Epoch 16/200\n",
      "90000/90000 [==============================] - 29s 321us/step - loss: 0.0887 - val_loss: 0.1894\n",
      "Epoch 17/200\n",
      " 2050/90000 [..............................] - ETA: 28s - loss: 0.0867"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0502ab80df07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 validation_data=(x_val, y_val),callbacks=[TensorBoard(log_dir='/tmp/autoencod')])\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# model.fit(x_train, y_train, epochs=100, verbose=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mdictData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RRII'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/haha/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda2/envs/haha/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda2/envs/haha/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/haha/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/haha/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/haha/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/haha/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda2/envs/haha/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/haha/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "from sklearn.cross_validation import train_test_split\n",
    "model5layers2 = Sequential()\n",
    "# activation=keras.layers.ReLU()\n",
    "model5layers2.add(Dense(600, input_dim=24, activation='relu'))\n",
    "model5layers2.add(Dense(480, activation='relu'))\n",
    "model5layers2.add(Dense(240, activation='relu'))\n",
    "model5layers2.add(Dense(120, activation='relu'))\n",
    "model5layers2.add(Dense(60, activation='relu'))\n",
    "model5layers2.add(Dense(24, activation='linear'))\n",
    "model5layers2.summary()\n",
    "adam=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model5layers2.compile(loss='mse', optimizer=adam)\n",
    "hist=model5layers2.fit(x_train, y_train,\n",
    "                epochs=200,\n",
    "                batch_size=50,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_val, y_val),callbacks=[TensorBoard(log_dir='/tmp/autoencod')])\n",
    "# model.fit(x_train, y_train, epochs=100, verbose=0)\n",
    "dictData['RRII']=hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_whole(dictData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DateLocation='MIMOComplex100000_3.mat'\n",
    "Data = sio.loadmat(DateLocation)#load data\n",
    "Xdata=Data['X']\n",
    "Ydata=Data['Y']\n",
    "Xdata = np.transpose(Xdata)\n",
    "Ydata = np.transpose(Ydata)\n",
    "X = np.column_stack((np.real(Xdata), np.imag(Xdata)))\n",
    "Y = np.column_stack((np.real(Ydata), np.imag(Ydata)))\n",
    "x_train, x_val ,y_train, y_val= train_test_split(X, Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "from sklearn.cross_validation import train_test_split\n",
    "model5layers5 = Sequential()\n",
    "# activation=keras.layers.ReLU()\n",
    "model5layers5.add(Dense(600, input_dim=24, activation='relu'))\n",
    "model5layers5.add(Dense(480, activation='relu'))\n",
    "model5layers5.add(Dense(240, activation='relu'))\n",
    "model5layers5.add(Dense(120, activation='relu'))\n",
    "model5layers5.add(Dense(60, activation='relu'))\n",
    "model5layers5.add(Dense(24, activation='linear'))\n",
    "model5layers5.summary()\n",
    "adam=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model5layers5.compile(loss='mse', optimizer=adam)\n",
    "hist=model5layers5.fit(x_train, y_train,\n",
    "                epochs=200,\n",
    "                batch_size=50,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_val, y_val),callbacks=[TensorBoard(log_dir='/tmp/autoencod')])\n",
    "# model.fit(x_train, y_train, epochs=100, verbose=0)\n",
    "dictData['RIRI']=hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "from keras.models import model_from_json\n",
    "model_json = model4layers.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model4layers.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(Xtest, Ytest, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
